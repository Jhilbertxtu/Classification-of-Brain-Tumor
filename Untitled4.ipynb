{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.667471\n",
      "Cost after iteration 3: -4.841158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:285: RuntimeWarning: divide by zero encountered in log\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:367: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:367: RuntimeWarning: invalid value encountered in true_divide\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:119: RuntimeWarning: invalid value encountered in multiply\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:98: RuntimeWarning: invalid value encountered in less_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 6: nan\n",
      "Cost after iteration 9: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdEAAAGHCAYAAADvFGhxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQkSf8H7nrN+9atb/sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb28037ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.412244897959\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import glob\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "path = 'Untitled Folder/*.mat'   \n",
    "files=glob.glob(path)\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "\n",
    "for file in files:\n",
    "    with h5py.File(file,'r') as f:\n",
    "        variables=f.items()\n",
    "        for var in variables:\n",
    "            name= var[0]\n",
    "            data=var[1]\n",
    "            X.append(np.array((data['tumorMask'].value.flatten())))\n",
    "            Y.append(data['label'].value.flatten())\n",
    "\n",
    "\n",
    "X=np.array(X)\n",
    "#print(type(X))\n",
    "Y=np.array(Y)\n",
    "\n",
    "\n",
    "def splittesttrain(X,Y):\n",
    "\tX_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=0)\n",
    "\treturn X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=splittesttrain(X,Y)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters     \n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 10, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches =  L_model_forward(X,parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL,Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL,Y,caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 3 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 3 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "layers_dims = [262144, 4, 1] #  3-layer model\n",
    "parameters = L_layer_model(X_train.T, Y_train.T, layers_dims, num_iterations = 10, print_cost = True)\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 3\n",
    "        else:\n",
    "            p[0,i] = 1\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)-0.2))\n",
    "        \n",
    "    return p\n",
    "\n",
    "p=predict(X_test.T, Y_test.T, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
