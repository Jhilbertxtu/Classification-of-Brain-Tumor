{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.694193\n",
      "Cost after iteration 3: 0.624958\n",
      "Cost after iteration 6: 0.265272\n",
      "Cost after iteration 9: -0.250314\n",
      "Cost after iteration 12: -1.023485\n",
      "Cost after iteration 15: -2.380057\n",
      "Cost after iteration 18: -5.642520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:285: RuntimeWarning: divide by zero encountered in log\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:367: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:119: RuntimeWarning: invalid value encountered in multiply\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:98: RuntimeWarning: invalid value encountered in less_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 21: nan\n",
      "Cost after iteration 24: nan\n",
      "Cost after iteration 27: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcoAAAGHCAYAAAAwWhJuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFgGLzJEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b026cef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.612244897959\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import glob\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "path = 'Untitled Folder/*.mat'   \n",
    "files=glob.glob(path)\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "\n",
    "for file in files:\n",
    "    with h5py.File(file,'r') as f:\n",
    "        variables=f.items()\n",
    "        for var in variables:\n",
    "            name= var[0]\n",
    "            data=var[1]\n",
    "            X.append(np.array((data['tumorMask'].value.flatten())))\n",
    "            Y.append(data['label'].value.flatten())\n",
    "\n",
    "\n",
    "X=np.array(X)\n",
    "#print(type(X))\n",
    "Y=np.array(Y)\n",
    "\n",
    "\n",
    "def splittesttrain(X,Y):\n",
    "\tX_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=0)\n",
    "\treturn X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=splittesttrain(X,Y)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters     \n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 30, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches =  L_model_forward(X,parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL,Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL,Y,caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 3 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 3 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "layers_dims = [262144, 20, 7, 5, 1] #  5-layer model\n",
    "parameters = L_layer_model(X_train.T, Y_train.T, layers_dims, num_iterations = 30, print_cost = True)\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 3\n",
    "        else:\n",
    "            p[0,i] = 1\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p\n",
    "\n",
    "p=predict(X_test.T, Y_test.T, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
