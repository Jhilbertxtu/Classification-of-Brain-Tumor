{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "\n",
      "[[57  3]\n",
      " [ 6 32]]\n",
      "Accuracy:0.91\n",
      "291\n",
      "291\n",
      "Done\n",
      "(291, 4)\n",
      "(4, 262144)\n",
      "(291, 1)\n",
      "Cost after iteration 0:0.528767\n",
      "(291, 4)\n",
      "(4, 262144)\n",
      "(291, 1)\n",
      "Cost after iteration 1:0.515071\n",
      "(291, 4)\n",
      "(4, 262144)\n",
      "(291, 1)\n",
      "Cost after iteration 2:0.501426\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi0AAAGHCAYAAAB8hmJnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngqFFkiR1gqFFkiR1wv8HcqKOXHjPaFoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc103b9c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "W1 = [[-0.0040159  -0.00027213 -0.02131525 ..., -0.01145175  0.01245699\n",
      "   0.01031203]\n",
      " [-0.00738977  0.01900404 -0.01076062 ...,  0.00871756  0.00370044\n",
      "  -0.00420394]\n",
      " [ 0.00126224 -0.00520544  0.0248978  ..., -0.00498372 -0.00943171\n",
      "  -0.00234647]\n",
      " [ 0.01573888  0.01973635  0.01401942 ..., -0.01061362  0.00269129\n",
      "   0.01006112]]\n",
      "b1 = [[ 0.00914494]\n",
      " [-0.01029804]\n",
      " [ 0.00507919]\n",
      " [ 0.0035459 ]]\n",
      "W2 = [[  7.47905823e-03   4.89549819e-03  -1.05890961e-02   7.07346261e-04]\n",
      " [ -1.16888347e-02  -8.83338143e-03   3.33189229e-03  -1.09386253e-02]\n",
      " [  4.23316639e-03  -3.88485841e-03   1.37995757e-02  -2.04395444e-03]\n",
      " ..., \n",
      " [ -6.68663562e-03  -2.94346990e-03   1.16916071e-02   3.00836371e-03]\n",
      " [ -3.32562824e-03  -4.39469815e-03  -9.13128588e-03   8.65462080e-03]\n",
      " [ -8.80551048e-03   9.31602021e-05   6.90624284e-04  -1.70265123e-02]]\n",
      "b2 = [[ 0.01122919]\n",
      " [ 0.01122839]\n",
      " [ 0.05614467]\n",
      " [ 0.05614507]\n",
      " [ 0.01122872]\n",
      " [ 0.01122817]\n",
      " [ 0.01122854]\n",
      " [ 0.01122825]\n",
      " [ 0.01122846]\n",
      " [ 0.01122905]\n",
      " [ 0.01122876]\n",
      " [ 0.01122885]\n",
      " [ 0.01122894]\n",
      " [ 0.01122988]\n",
      " [ 0.05614457]\n",
      " [ 0.05614485]\n",
      " [ 0.05614455]\n",
      " [ 0.0112289 ]\n",
      " [ 0.05614539]\n",
      " [ 0.05614416]\n",
      " [ 0.01122842]\n",
      " [ 0.01122884]\n",
      " [ 0.05614434]\n",
      " [ 0.01122959]\n",
      " [ 0.05614421]\n",
      " [ 0.01122878]\n",
      " [ 0.01122897]\n",
      " [ 0.05614395]\n",
      " [ 0.0561446 ]\n",
      " [ 0.05614543]\n",
      " [ 0.01122928]\n",
      " [ 0.056145  ]\n",
      " [ 0.01122879]\n",
      " [ 0.01122831]\n",
      " [ 0.01122896]\n",
      " [ 0.05614461]\n",
      " [ 0.01122878]\n",
      " [ 0.01122917]\n",
      " [ 0.05614448]\n",
      " [ 0.01122931]\n",
      " [ 0.01122913]\n",
      " [ 0.05614381]\n",
      " [ 0.01122883]\n",
      " [ 0.01122921]\n",
      " [ 0.05614495]\n",
      " [ 0.01122926]\n",
      " [ 0.01122879]\n",
      " [ 0.01122988]\n",
      " [ 0.01122858]\n",
      " [ 0.01122913]\n",
      " [ 0.0112286 ]\n",
      " [ 0.01122905]\n",
      " [ 0.01122888]\n",
      " [ 0.05614512]\n",
      " [ 0.01122878]\n",
      " [ 0.01122831]\n",
      " [ 0.05614424]\n",
      " [ 0.01122913]\n",
      " [ 0.01122838]\n",
      " [ 0.0112284 ]\n",
      " [ 0.05614452]\n",
      " [ 0.01122922]\n",
      " [ 0.01122863]\n",
      " [ 0.05614463]\n",
      " [ 0.05614375]\n",
      " [ 0.0561453 ]\n",
      " [ 0.01122938]\n",
      " [ 0.01122961]\n",
      " [ 0.01122886]\n",
      " [ 0.01122865]\n",
      " [ 0.01122954]\n",
      " [ 0.01122904]\n",
      " [ 0.0561447 ]\n",
      " [ 0.0112289 ]\n",
      " [ 0.01122839]\n",
      " [ 0.05614458]\n",
      " [ 0.01122911]\n",
      " [ 0.05614443]\n",
      " [ 0.05614447]\n",
      " [ 0.01122845]\n",
      " [ 0.01122874]\n",
      " [ 0.05614466]\n",
      " [ 0.01122991]\n",
      " [ 0.01122846]\n",
      " [ 0.05614474]\n",
      " [ 0.01122836]\n",
      " [ 0.05614425]\n",
      " [ 0.01122887]\n",
      " [ 0.05614483]\n",
      " [ 0.01122881]\n",
      " [ 0.05614455]\n",
      " [ 0.01122871]\n",
      " [ 0.01122822]\n",
      " [ 0.05614409]\n",
      " [ 0.01122873]\n",
      " [ 0.01122838]\n",
      " [ 0.05614469]\n",
      " [ 0.01122883]\n",
      " [ 0.01122869]\n",
      " [ 0.01122797]\n",
      " [ 0.0112287 ]\n",
      " [ 0.01122854]\n",
      " [ 0.05614464]\n",
      " [ 0.01122939]\n",
      " [ 0.01122902]\n",
      " [ 0.0112281 ]\n",
      " [ 0.01122894]\n",
      " [ 0.01122884]\n",
      " [ 0.05614436]\n",
      " [ 0.0112284 ]\n",
      " [ 0.0112287 ]\n",
      " [ 0.0561437 ]\n",
      " [ 0.01122854]\n",
      " [ 0.01122921]\n",
      " [ 0.05614396]\n",
      " [ 0.01122916]\n",
      " [ 0.01122945]\n",
      " [ 0.05614476]\n",
      " [ 0.01122816]\n",
      " [ 0.01122982]\n",
      " [ 0.01122881]\n",
      " [ 0.05614397]\n",
      " [ 0.01122894]\n",
      " [ 0.05614479]\n",
      " [ 0.05614454]\n",
      " [ 0.05614474]\n",
      " [ 0.01122884]\n",
      " [ 0.05614474]\n",
      " [ 0.05614503]\n",
      " [ 0.011228  ]\n",
      " [ 0.01122942]\n",
      " [ 0.05614437]\n",
      " [ 0.01122802]\n",
      " [ 0.01122967]\n",
      " [ 0.05614466]\n",
      " [ 0.01122899]\n",
      " [ 0.01122914]\n",
      " [ 0.01122939]\n",
      " [ 0.05614446]\n",
      " [ 0.0561446 ]\n",
      " [ 0.05614511]\n",
      " [ 0.01122859]\n",
      " [ 0.05614544]\n",
      " [ 0.01122825]\n",
      " [ 0.01122901]\n",
      " [ 0.01122882]\n",
      " [ 0.05614418]\n",
      " [ 0.05614566]\n",
      " [ 0.01122877]\n",
      " [ 0.01122881]\n",
      " [ 0.05614452]\n",
      " [ 0.05614447]\n",
      " [ 0.01122862]\n",
      " [ 0.05614534]\n",
      " [ 0.05614443]\n",
      " [ 0.05614522]\n",
      " [ 0.05614536]\n",
      " [ 0.05614491]\n",
      " [ 0.05614395]\n",
      " [ 0.05614506]\n",
      " [ 0.01122901]\n",
      " [ 0.05614453]\n",
      " [ 0.01122888]\n",
      " [ 0.05614418]\n",
      " [ 0.05614457]\n",
      " [ 0.05614463]\n",
      " [ 0.01122852]\n",
      " [ 0.05614433]\n",
      " [ 0.0112287 ]\n",
      " [ 0.01122988]\n",
      " [ 0.01122898]\n",
      " [ 0.01122862]\n",
      " [ 0.01122905]\n",
      " [ 0.05614482]\n",
      " [ 0.01122785]\n",
      " [ 0.01122893]\n",
      " [ 0.05614464]\n",
      " [ 0.01122972]\n",
      " [ 0.01122895]\n",
      " [ 0.0561443 ]\n",
      " [ 0.0112281 ]\n",
      " [ 0.05614416]\n",
      " [ 0.01122892]\n",
      " [ 0.05614497]\n",
      " [ 0.01122928]\n",
      " [ 0.01122855]\n",
      " [ 0.01122821]\n",
      " [ 0.01122855]\n",
      " [ 0.01122907]\n",
      " [ 0.01122972]\n",
      " [ 0.01122893]\n",
      " [ 0.01122742]\n",
      " [ 0.01122956]\n",
      " [ 0.01122887]\n",
      " [ 0.05614539]\n",
      " [ 0.01122919]\n",
      " [ 0.05614541]\n",
      " [ 0.05614496]\n",
      " [ 0.05614409]\n",
      " [ 0.01122845]\n",
      " [ 0.01122878]\n",
      " [ 0.01122845]\n",
      " [ 0.01122837]\n",
      " [ 0.01122872]\n",
      " [ 0.01122906]\n",
      " [ 0.01122896]\n",
      " [ 0.01122914]\n",
      " [ 0.01122925]\n",
      " [ 0.0561449 ]\n",
      " [ 0.01122969]\n",
      " [ 0.01122923]\n",
      " [ 0.05614438]\n",
      " [ 0.01122894]\n",
      " [ 0.05614456]\n",
      " [ 0.01122943]\n",
      " [ 0.01122885]\n",
      " [ 0.01122958]\n",
      " [ 0.0561448 ]\n",
      " [ 0.05614408]\n",
      " [ 0.01122883]\n",
      " [ 0.05614455]\n",
      " [ 0.01122883]\n",
      " [ 0.01122947]\n",
      " [ 0.01122928]\n",
      " [ 0.05614496]\n",
      " [ 0.01122876]\n",
      " [ 0.01122928]\n",
      " [ 0.01122977]\n",
      " [ 0.01122822]\n",
      " [ 0.01122975]\n",
      " [ 0.01122886]\n",
      " [ 0.01122886]\n",
      " [ 0.01122959]\n",
      " [ 0.05614513]\n",
      " [ 0.01122922]\n",
      " [ 0.01122801]\n",
      " [ 0.05614462]\n",
      " [ 0.01122842]\n",
      " [ 0.01122914]\n",
      " [ 0.01122902]\n",
      " [ 0.01122805]\n",
      " [ 0.01122911]\n",
      " [ 0.01122896]\n",
      " [ 0.01122882]\n",
      " [ 0.05614476]\n",
      " [ 0.05614465]\n",
      " [ 0.05614401]\n",
      " [ 0.056144  ]\n",
      " [ 0.01122873]\n",
      " [ 0.01122962]\n",
      " [ 0.01122927]\n",
      " [ 0.01122883]\n",
      " [ 0.01122864]\n",
      " [ 0.01122795]\n",
      " [ 0.05614426]\n",
      " [ 0.0561448 ]\n",
      " [ 0.05614446]\n",
      " [ 0.01122945]\n",
      " [ 0.05614448]\n",
      " [ 0.01122895]\n",
      " [ 0.01122797]\n",
      " [ 0.01122958]\n",
      " [ 0.0561448 ]\n",
      " [ 0.01122857]\n",
      " [ 0.01122871]\n",
      " [ 0.01122882]\n",
      " [ 0.05614403]\n",
      " [ 0.01122842]\n",
      " [ 0.01122925]\n",
      " [ 0.05614386]\n",
      " [ 0.05614419]\n",
      " [ 0.01122968]\n",
      " [ 0.01122883]\n",
      " [ 0.05614445]\n",
      " [ 0.01122885]\n",
      " [ 0.05614352]\n",
      " [ 0.05614402]\n",
      " [ 0.01122868]\n",
      " [ 0.01122934]\n",
      " [ 0.01122905]\n",
      " [ 0.01122955]\n",
      " [ 0.01122878]\n",
      " [ 0.05614507]\n",
      " [ 0.0561445 ]\n",
      " [ 0.05614527]\n",
      " [ 0.05614457]\n",
      " [ 0.01122924]\n",
      " [ 0.01122918]\n",
      " [ 0.01122875]\n",
      " [ 0.01122861]\n",
      " [ 0.01122876]]\n",
      "(98, 4)\n",
      "(4, 262144)\n",
      "(98, 1)\n",
      "predictions mean = 0.507208038853\n",
      "Accuracy:61%\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "path = 'Untitled Folder/*.mat'   \n",
    "files=glob.glob(path)\n",
    "#os.remove('E:/finalProject/'+'1.mat')\n",
    "\n",
    "#X=np.zeros((1638,262144))\n",
    "X=[]\n",
    "Y=[]\n",
    "\n",
    "for file in files:\n",
    "    with h5py.File(file,'r') as f:\n",
    "    #f=h5py.File(file,'r')\n",
    "        variables=f.items()\n",
    "        for var in variables:\n",
    "            name= var[0]\n",
    "            data=var[1]\n",
    "            #print(data['tumorMask'].value)\n",
    "            #X[i]=data['tumorMask'].value\n",
    "            #Y[i]=data['label'].value\n",
    "            #if v == 2:\n",
    "            #    f.close()\n",
    "            #    os.remove(file)\n",
    "            #print(v)\n",
    "            #flat=np.array(data['tumorMask'].value.flatten())\n",
    "            #X[i,]=flat\n",
    "            \"\"\"k=0\n",
    "            for j in flat:\n",
    "                X[i][k]=j\n",
    "                print(X[i][k])\n",
    "                k=k+1\n",
    "            print('Done')\n",
    "            \"\"\"\n",
    "            #X=np.concatenate([np.array(i) for i in flat])\n",
    "            X.append(np.array((data['tumorMask'].value.flatten())))\n",
    "            Y.append(data['label'].value.flatten())\n",
    "#            i=i+1\n",
    "\n",
    "#print(X[0])\n",
    "#print(Y[0])\n",
    "\n",
    "X=np.array(X)\n",
    "print(type(X))\n",
    "Y=np.array(Y)\n",
    "\n",
    "#data=np.loadtxt('ex2data1.txt', delimiter=',')\n",
    "def splittesttrain(X,Y):\n",
    "\t#X = data[:, 0:2]\n",
    "\t#Y = data[:, 2]\n",
    "\tX_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=0)\n",
    "\t#X_train.shape\n",
    "\treturn X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def applyLR(X_train,Y_train):\n",
    "\tclassifier=LogisticRegression(random_state=0)\n",
    "\tclassifier.fit(X_train,Y_train)\n",
    "\treturn classifier\n",
    "\t\t\n",
    "def predict(X_test, Y_test, classifier):\n",
    "\ty_pred=classifier.predict(X_test)\n",
    "\tfrom sklearn.metrics import confusion_matrix\n",
    "\tconfusion_matrix=confusion_matrix(Y_test,y_pred)\n",
    "\tprint(\"Confusion matrix:\\n\")\n",
    "\tprint(confusion_matrix)\n",
    "\n",
    "def testAccuracy(X_test,Y_test,classifier):\n",
    "\tprint(\"Accuracy:{:.2f}\".format(classifier.score(X_test,Y_test)))\n",
    "\t\n",
    "X_train,X_test,Y_train,Y_test=splittesttrain(X,Y)\n",
    "classifier=applyLR(X_train,Y_train)\n",
    "predict(X_test,Y_test,classifier)\n",
    "testAccuracy(X_test,Y_test,classifier)\n",
    "\n",
    "\n",
    "#Neural Netwok\n",
    "\"\"\"\n",
    "#test_cases\n",
    "def layer_sizes_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(5, 3)\n",
    "    Y_assess = np.random.randn(2, 3)\n",
    "    return X_assess, Y_assess\n",
    "\n",
    "def initialize_parameters_test_case():\n",
    "    n_x, n_h, n_y = 2, 4, 1\n",
    "    return n_x, n_h, n_y\n",
    "\n",
    "\n",
    "def forward_propagation_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(2, 3)\n",
    "    b1 = np.random.randn(4,1)\n",
    "    b2 = np.array([[ -1.3]])\n",
    "\n",
    "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
    "        [-0.02136196,  0.01640271],\n",
    "        [-0.01793436, -0.00841747],\n",
    "        [ 0.00502881, -0.01245288]]),\n",
    "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
    "     'b1': b1,\n",
    "     'b2': b2}\n",
    "\n",
    "    return X_assess, parameters\n",
    "\n",
    "def compute_cost_test_case():\n",
    "    np.random.seed(1)\n",
    "    Y_assess = (np.random.randn(1, 3) > 0)\n",
    "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
    "        [-0.02136196,  0.01640271],\n",
    "        [-0.01793436, -0.00841747],\n",
    "        [ 0.00502881, -0.01245288]]),\n",
    "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
    "     'b1': np.array([[ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.]]),\n",
    "     'b2': np.array([[ 0.]])}\n",
    "\n",
    "    a2 = (np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]))\n",
    "    \n",
    "    return a2, Y_assess, parameters\n",
    "\n",
    "def backward_propagation_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(2, 3)\n",
    "    Y_assess = (np.random.randn(1, 3) > 0)\n",
    "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
    "        [-0.02136196,  0.01640271],\n",
    "        [-0.01793436, -0.00841747],\n",
    "        [ 0.00502881, -0.01245288]]),\n",
    "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
    "     'b1': np.array([[ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.]]),\n",
    "     'b2': np.array([[ 0.]])}\n",
    "\n",
    "    cache = {'A1': np.array([[-0.00616578,  0.0020626 ,  0.00349619],\n",
    "         [-0.05225116,  0.02725659, -0.02646251],\n",
    "         [-0.02009721,  0.0036869 ,  0.02883756],\n",
    "         [ 0.02152675, -0.01385234,  0.02599885]]),\n",
    "  'A2': np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]),\n",
    "  'Z1': np.array([[-0.00616586,  0.0020626 ,  0.0034962 ],\n",
    "         [-0.05229879,  0.02726335, -0.02646869],\n",
    "         [-0.02009991,  0.00368692,  0.02884556],\n",
    "         [ 0.02153007, -0.01385322,  0.02600471]]),\n",
    "  'Z2': np.array([[ 0.00092281, -0.00056678,  0.00095853]])}\n",
    "    return parameters, cache, X_assess, Y_assess\n",
    "\n",
    "def update_parameters_test_case():\n",
    "    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n",
    "        [-0.02311792,  0.03137121],\n",
    "        [-0.0169217 , -0.01752545],\n",
    "        [ 0.00935436, -0.05018221]]),\n",
    " 'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
    " 'b1': np.array([[ -8.97523455e-07],\n",
    "        [  8.15562092e-06],\n",
    "        [  6.04810633e-07],\n",
    "        [ -2.54560700e-06]]),\n",
    " 'b2': np.array([[  9.14954378e-05]])}\n",
    "\n",
    "    grads = {'dW1': np.array([[ 0.00023322, -0.00205423],\n",
    "        [ 0.00082222, -0.00700776],\n",
    "        [-0.00031831,  0.0028636 ],\n",
    "        [-0.00092857,  0.00809933]]),\n",
    " 'dW2': np.array([[ -1.75740039e-05,   3.70231337e-03,  -1.25683095e-03,\n",
    "          -2.55715317e-03]]),\n",
    " 'db1': np.array([[  1.05570087e-07],\n",
    "        [ -3.81814487e-06],\n",
    "        [ -1.90155145e-07],\n",
    "        [  5.46467802e-07]]),\n",
    " 'db2': np.array([[ -1.08923140e-05]])}\n",
    "    return parameters, grads\n",
    "\n",
    "def nn_model_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(2, 3)\n",
    "    Y_assess = (np.random.randn(1, 3) > 0)\n",
    "    return X_assess, Y_assess\n",
    "\n",
    "def predict_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(2, 3)\n",
    "    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n",
    "        [-0.02311792,  0.03137121],\n",
    "        [-0.0169217 , -0.01752545],\n",
    "        [ 0.00935436, -0.05018221]]),\n",
    "     'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
    "     'b1': np.array([[ -8.97523455e-07],\n",
    "        [  8.15562092e-06],\n",
    "        [  6.04810633e-07],\n",
    "        [ -2.54560700e-06]]),\n",
    "     'b2': np.array([[  9.14954378e-05]])}\n",
    "    return parameters, X_assess\n",
    "\"\"\"\n",
    "#planar\n",
    "def sigmoid(x):\n",
    "    s=1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "#main\n",
    "def layer_sizes(X,Y):\n",
    "    n_x=X.shape[0]\n",
    "    n_h=4\n",
    "    n_y=Y.shape[0]\n",
    "    return (n_x, n_h, n_y)\n",
    "#X_assess, Y_assess=layer_sizes_test_case()\n",
    "(n_x, n_h, n_y)=layer_sizes(X_train, Y_train)\n",
    "print (n_x)\n",
    "print (n_y)\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(2)\n",
    "    W1=np.random.randn(n_h,n_x)*0.01\n",
    "    b1=np.zeros((n_h,1))\n",
    "    W2=np.random.randn(n_y,n_h)*0.01\n",
    "    b2=np.zeros((n_y,1))\n",
    "    parameters={\"W1\":W1,\"b1\":b1,\"W2\":W2,\"b2\":b2}\n",
    "    return parameters\n",
    "\n",
    "#n_x, n_h, n_y = initialize_parameters_test_case()\n",
    "\n",
    "#parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\"\"\"print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "\"\"\"\n",
    "def forward_propagation(X,parameters):\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "\n",
    "    Z1=np.dot(W1,X)+b1\n",
    "    A1=np.tanh(Z1)\n",
    "    print (W2.shape)\n",
    "    print (A1.shape)\n",
    "    print(b2.shape)\n",
    "    #g=np.dot(W2,A1)\n",
    "    #print (g.shape)\n",
    "    Z2=np.dot(W2,A1)+b2\n",
    "    '''# explicit for loops\n",
    "    res=[]\n",
    "    for i in range(len(W2)):\n",
    "        for j in range(len(A1[0])):\n",
    "            res[i][j]=0\n",
    "            for k in range(len(A1)):\n",
    " \n",
    "                 # resulted matrix\n",
    "                 res[i][j]= res[i][j]+(W2[i][k] * A1[k][j])\n",
    "    res=res+b\n",
    "    Z2=res'''\n",
    "    #print (A2.shape)\n",
    "    A2=sigmoid(Z2)\n",
    "\n",
    "    cache={\"Z1\":Z1,\"A1\":A1,\"Z2\":Z2,\"A2\":A2}\n",
    "    return A2,cache\n",
    "\n",
    "#X_assess, parameters = forward_propagation_test_case()\n",
    "#A2, cache = forward_propagation(X_train, parameters)\n",
    "\n",
    "# Note: we use the mean here just to make sure that your output matches ours. \n",
    "#print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))\n",
    "\n",
    "\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    m=Y.shape[1]\n",
    "    #print m\n",
    "    logprobs=np.multiply(np.log(A2),Y)+np.multiply(np.log(1-A2),(1-Y))\n",
    "    cost= -np.sum(logprobs)/m\n",
    "    cost=cost/100000000\n",
    "    return cost\n",
    "#A2, Y_assess, parameters = compute_cost_test_case()\n",
    "\n",
    "#print(\"cost = \" + str(compute_cost(A2, Y_train, parameters)))\n",
    "    \n",
    "def backward_propagation(parameters,cache,X,Y):\n",
    "    m=X.shape[1]\n",
    "    W1=parameters[\"W1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    A1=cache[\"A1\"]\n",
    "    A2=cache[\"A2\"]\n",
    "    dZ2=A2-Y\n",
    "    dW2=np.dot(dZ2,A1.T)/m\n",
    "    db2=np.sum(dZ2,axis=1,keepdims=True)/m\n",
    "    dZ1=np.dot(W2.T,dZ2)*(1-np.power(A1,2))\n",
    "    dW1=np.dot(dZ1,X.T)/m\n",
    "    db1=np.sum(dZ1,axis=1,keepdims=True)/m\n",
    "\n",
    "    grads={\"dW1\":dW1,\"db1\":db1,\"dW2\":dW2,\"db2\":db2}\n",
    "    return grads\n",
    "\n",
    "#parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n",
    "\n",
    "#grads = backward_propagation(parameters, cache, X_train, Y_train)\n",
    "\"\"\"\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print (\"db2 = \"+ str(grads[\"db2\"]))\n",
    "\"\"\"\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate=0.0075):\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    dW1=grads[\"dW1\"]\n",
    "    db1=grads[\"db1\"]\n",
    "    dW2=grads[\"dW2\"]\n",
    "    db2=grads[\"db2\"]\n",
    "\n",
    "    W1=W1-learning_rate*dW1\n",
    "    b1=b1-learning_rate*db1\n",
    "    W2=W2-learning_rate*dW2\n",
    "    b2=b2-learning_rate*db2\n",
    "\n",
    "    parameters={\"W1\":W1,\"b1\":b1,\"W2\":W2,\"b2\":b2}\n",
    "    return parameters\n",
    "\n",
    "#parameters, grads = update_parameters_test_case()\n",
    "#parameters = update_parameters(parameters, grads)\n",
    "\"\"\"\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\"\"\"\n",
    "\n",
    "def nn_model(X,Y,n_h,num_iterations=5,print_cost=False):\n",
    "    np.random.seed(3)\n",
    "    n_x=layer_sizes(X,Y)[0]\n",
    "    n_y=layer_sizes(X,Y)[2]\n",
    "\n",
    "    parameters=initialize_parameters(n_x,n_h,n_y)\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    for i in range(0,num_iterations):\n",
    "        A2,cache=forward_propagation(X,parameters)\n",
    "        cost=compute_cost(A2,Y,parameters)\n",
    "        grads=backward_propagation(parameters,cache,X,Y)\n",
    "        parameters=update_parameters(parameters,grads)\n",
    "        if print_cost and i%1==0:\n",
    "            print (\"Cost after iteration %i:%f\" %(i,cost))\n",
    "\n",
    "    learning_rate=0.0075\n",
    "    plt.plot(cost)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters\n",
    "#X_assess, Y_assess = nn_model_test_case()\n",
    "print (\"Done\")\n",
    "parameters = nn_model(X_train, Y_train, 4, num_iterations=3, print_cost=True)\n",
    "print (\"Hi\")\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "\n",
    "def predict(parameters,X):\n",
    "    n_x,n_h,n_y=layer_sizes(X_test,Y_test)\n",
    "    parameters=initialize_parameters(n_x,n_h,n_y)\n",
    "    A2,cache=forward_propagation(X,parameters)\n",
    "    cost=compute_cost(A2,Y_test,parameters)\n",
    "    grads=backward_propagation(parameters,cache,X,Y_test)\n",
    "    parameters=update_parameters(parameters,grads)\n",
    "    b2=parameters[\"b2\"]\n",
    "    p=b2\n",
    "    m = X.shape[1]\n",
    "    #print (A2)\n",
    "    #p[p>2]=3\n",
    "    #p[p<2]=1\n",
    "    #predictions=p\n",
    "    #print (Y_test.shape)\n",
    "    #print (predictions.shape)\n",
    "    for i in range(0, A2.shape[1]):\n",
    "        if A2[0,i] > 0.5:\n",
    "            A2[0,i] = 3\n",
    "        else:\n",
    "            A2[0,i] = 1\n",
    "    \n",
    "    predictions=A2\n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    #print(\"Accuracy: \"  + str(np.sum((A2 == Y_test)/m)))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "#parameters, X_assess = predict_test_case()\n",
    "\n",
    "predictions = predict(parameters, X_test)\n",
    "#print (predictions)\n",
    "#print (Y_test)\n",
    "print(\"predictions mean = \" + str(np.mean(predictions)))\n",
    "\n",
    "\n",
    "#parameters = nn_model(X_train, Y_train, n_h = 4, num_iterations = 10000, print_cost=True)\n",
    "    \n",
    "# Print accuracy\n",
    "#predictions = predict(parameters, X_test)\n",
    "#print ('Accuracy: %d' % float(np.mean((np.dot(Y_test.T,predictions) + np.dot(1-Y_test.T,1-predictions))/float(Y_test.size)*100)) + '%')\n",
    "#print('Accuracy:%d'%float(np.mean(predictions)*100)+'%')\n",
    "acc=0\n",
    "for i in range(Y_test.size):\n",
    "    if(predictions[0,i]==Y_test[i]):\n",
    "        acc=acc+1\n",
    "#    else:\n",
    "#        acc=acc-1\n",
    "\n",
    "accuracy=(acc/float(Y_test.size))*100\n",
    "\n",
    "print ('Accuracy:%d'%float(accuracy)+'%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
